{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ace4699",
   "metadata": {},
   "source": [
    "# Fine-Tuning ChemBERTa for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614ed41-7f65-4495-aa3d-8fba22013597",
   "metadata": {},
   "source": [
    "Different learning rates, number of epochs, and batch sizes were explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff167f-c177-4f9f-9408-bd499f2d63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d30e92-0aea-498c-8d3f-d8b24cf4aeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 133055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PBE_TZP</th>\n",
       "      <th>B3LYP(VWN5)_TZP</th>\n",
       "      <th>GFNXTB</th>\n",
       "      <th>M06-2X_TZP</th>\n",
       "      <th>smiles</th>\n",
       "      <th>chemformula</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.261372</td>\n",
       "      <td>-19.140565</td>\n",
       "      <td>-21.979934</td>\n",
       "      <td>-19.987828</td>\n",
       "      <td>[H]C([H])([H])[H]</td>\n",
       "      <td>CH4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.975380</td>\n",
       "      <td>-12.896428</td>\n",
       "      <td>-17.405808</td>\n",
       "      <td>-12.694483</td>\n",
       "      <td>[H]N([H])[H]</td>\n",
       "      <td>H3N</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.370616</td>\n",
       "      <td>-11.226953</td>\n",
       "      <td>-14.033122</td>\n",
       "      <td>-12.538535</td>\n",
       "      <td>[H]O[H]</td>\n",
       "      <td>H2O</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-18.070231</td>\n",
       "      <td>-19.266542</td>\n",
       "      <td>-22.665365</td>\n",
       "      <td>-21.246587</td>\n",
       "      <td>[H]C#C[H]</td>\n",
       "      <td>C2H2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.127323</td>\n",
       "      <td>-14.386552</td>\n",
       "      <td>-19.345571</td>\n",
       "      <td>-15.292001</td>\n",
       "      <td>[H]C#N</td>\n",
       "      <td>CHN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PBE_TZP  B3LYP(VWN5)_TZP     GFNXTB  M06-2X_TZP             smiles  \\\n",
       "0 -18.261372       -19.140565 -21.979934  -19.987828  [H]C([H])([H])[H]   \n",
       "1 -12.975380       -12.896428 -17.405808  -12.694483       [H]N([H])[H]   \n",
       "2 -10.370616       -11.226953 -14.033122  -12.538535            [H]O[H]   \n",
       "3 -18.070231       -19.266542 -22.665365  -21.246587          [H]C#C[H]   \n",
       "4 -14.127323       -14.386552 -19.345571  -15.292001             [H]C#N   \n",
       "\n",
       "  chemformula  index  \n",
       "0         CH4      1  \n",
       "1         H3N      2  \n",
       "2         H2O      3  \n",
       "3        C2H2      4  \n",
       "4         CHN      5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/qm9_recommended_data.csv\")\n",
    "print(f'Number of examples is: {len(df)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04851bc1-5707-4bd5-b39d-0d5d618dd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = tokenizer.model_max_length\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555dcb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def canonical_smiles(smiles):\n",
    "    canonicalized_smiles = []\n",
    "\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            # Create a molecule and sanitize it\n",
    "            mol = Chem.MolFromSmiles(smi, sanitize=False)\n",
    "            if mol is None:\n",
    "                raise ValueError(f\"Invalid SMILES: {smi}\")\n",
    "\n",
    "            # Sanitize the molecule\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "            # Convert to canonical SMILES\n",
    "            canonicalized_smiles.append(Chem.MolToSmiles(mol))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SMILES '{smi}': {e}\")\n",
    "            canonicalized_smiles.append(None)  # Append None for invalid SMILES\n",
    "\n",
    "    return canonicalized_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f37390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133055\n",
      "133055\n"
     ]
    }
   ],
   "source": [
    "# Runf the canonicalization\n",
    "canonicalized_smiles = canonical_smiles(df['smiles'].tolist())\n",
    "print(len(df))\n",
    "print(len(canonicalized_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28fee481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CANONICAL_SMILES\n",
    "df['CANONICAL_SMILES'] = canonicalized_smiles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755634da-6e15-46c3-a9d2-fdc7bd2aa75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/133055 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133055/133055 [00:12<00:00, 10931.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Tokenize and encode a string using the provided tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        string (str): Input string to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of input_ids and attention_mask.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        string,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Tokenize the 'CANONICAL_SMILES' column and create new columns 'input_ids' and 'attention_mask'\n",
    "tqdm.pandas()\n",
    "df[[\"input_ids\", \"attention_mask\"]] = df[\"smiles\"].progress_apply(lambda x: tokenize(x)).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec7d46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PBE_TZP</th>\n",
       "      <th>B3LYP(VWN5)_TZP</th>\n",
       "      <th>GFNXTB</th>\n",
       "      <th>M06-2X_TZP</th>\n",
       "      <th>smiles</th>\n",
       "      <th>chemformula</th>\n",
       "      <th>index</th>\n",
       "      <th>CANONICAL_SMILES</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.261372</td>\n",
       "      <td>-19.140565</td>\n",
       "      <td>-21.979934</td>\n",
       "      <td>-19.987828</td>\n",
       "      <td>[H]C([H])([H])[H]</td>\n",
       "      <td>CH4</td>\n",
       "      <td>1</td>\n",
       "      <td>[H]C([H])([H])[H]</td>\n",
       "      <td>[12, 16, 17, 18, 17, 18, 13, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.975380</td>\n",
       "      <td>-12.896428</td>\n",
       "      <td>-17.405808</td>\n",
       "      <td>-12.694483</td>\n",
       "      <td>[H]N([H])[H]</td>\n",
       "      <td>H3N</td>\n",
       "      <td>2</td>\n",
       "      <td>[H]N([H])[H]</td>\n",
       "      <td>[12, 23, 17, 18, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.370616</td>\n",
       "      <td>-11.226953</td>\n",
       "      <td>-14.033122</td>\n",
       "      <td>-12.538535</td>\n",
       "      <td>[H]O[H]</td>\n",
       "      <td>H2O</td>\n",
       "      <td>3</td>\n",
       "      <td>[H]O[H]</td>\n",
       "      <td>[12, 19, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-18.070231</td>\n",
       "      <td>-19.266542</td>\n",
       "      <td>-22.665365</td>\n",
       "      <td>-21.246587</td>\n",
       "      <td>[H]C#C[H]</td>\n",
       "      <td>C2H2</td>\n",
       "      <td>4</td>\n",
       "      <td>[H]C#C[H]</td>\n",
       "      <td>[12, 16, 38, 16, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.127323</td>\n",
       "      <td>-14.386552</td>\n",
       "      <td>-19.345571</td>\n",
       "      <td>-15.292001</td>\n",
       "      <td>[H]C#N</td>\n",
       "      <td>CHN</td>\n",
       "      <td>5</td>\n",
       "      <td>[H]C#N</td>\n",
       "      <td>[12, 16, 38, 23, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PBE_TZP  B3LYP(VWN5)_TZP     GFNXTB  M06-2X_TZP             smiles  \\\n",
       "0 -18.261372       -19.140565 -21.979934  -19.987828  [H]C([H])([H])[H]   \n",
       "1 -12.975380       -12.896428 -17.405808  -12.694483       [H]N([H])[H]   \n",
       "2 -10.370616       -11.226953 -14.033122  -12.538535            [H]O[H]   \n",
       "3 -18.070231       -19.266542 -22.665365  -21.246587          [H]C#C[H]   \n",
       "4 -14.127323       -14.386552 -19.345571  -15.292001             [H]C#N   \n",
       "\n",
       "  chemformula  index   CANONICAL_SMILES  \\\n",
       "0         CH4      1  [H]C([H])([H])[H]   \n",
       "1         H3N      2       [H]N([H])[H]   \n",
       "2         H2O      3            [H]O[H]   \n",
       "3        C2H2      4          [H]C#C[H]   \n",
       "4         CHN      5             [H]C#N   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [12, 16, 17, 18, 17, 18, 13, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [12, 23, 17, 18, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [12, 19, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [12, 16, 38, 16, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [12, 16, 38, 23, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fade54-27fa-4ff1-af69-ea8d96c4ec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 106444 molecules in Train df.\n",
      "There are 13305 molecules in Val df.\n",
      "There are 13306 molecules in Test df.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=21)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=21)\n",
    "print(f\"There are {len(train_df)} molecules in Train df.\")\n",
    "print(f\"There are {len(val_df)} molecules in Val df.\")\n",
    "print(f\"There are {len(test_df)} molecules in Test df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d9638c-b3fc-4063-8ea8-a1303178466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert data to PyTorch tensors\n",
    "def get_tensor_data(data):\n",
    "    \"\"\"\n",
    "    Convert data to PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): Input data containing 'input_ids', 'attention_mask', and 'pIC50' columns.\n",
    "    \n",
    "    Returns:\n",
    "        TensorDataset containing input_ids, attention_mask, and labels tensors.\n",
    "    \"\"\"\n",
    "    input_ids_tensor = torch.tensor(data[\"input_ids\"].tolist(), dtype=torch.int32)\n",
    "    attention_mask_tensor = torch.tensor(data[\"attention_mask\"].tolist(), dtype=torch.int32)\n",
    "    labels_tensor = torch.tensor(data[\"PBE_TZP\"].tolist(), dtype=torch.float32)\n",
    "    return TensorDataset(input_ids_tensor, attention_mask_tensor, labels_tensor)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = get_tensor_data(train_df)\n",
    "val_dataset = get_tensor_data(val_df)\n",
    "test_dataset = get_tensor_data(test_df)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ebfebf6-b6db-4082-ab9a-70b79884014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [10:28<1:34:19, 628.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: Train Loss 260.4838, Val Loss 120.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [21:02<1:24:11, 631.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: Train Loss 125.6776, Val Loss 120.8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [31:34<1:13:44, 632.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: Train Loss 138.2272, Val Loss 120.4558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [42:07<1:03:14, 632.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: Train Loss 125.7004, Val Loss 121.6456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [52:40<52:42, 632.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: Train Loss 125.6113, Val Loss 120.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [1:03:13<42:10, 632.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: Train Loss 125.4516, Val Loss 120.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [1:13:46<31:38, 632.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: Train Loss 125.8090, Val Loss 120.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:24:19<21:05, 632.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: Train Loss 125.9951, Val Loss 120.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:34:52<10:32, 632.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: Train Loss 125.9131, Val Loss 122.6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:45:24<00:00, 632.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: Train Loss 125.6061, Val Loss 120.5266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "#device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "torch.manual_seed(12345)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        output_dict = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = output_dict.logits.squeeze(dim=1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            output_dict = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            predictions = output_dict.logits.squeeze(dim=1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"epoch {epoch + 1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc36150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "      (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.144, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.109, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.144, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53edc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 119.8831\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "total_test_loss = 0\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        output_dict = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = output_dict.logits.squeeze(dim=1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        total_test_loss += loss.item()\n",
    "        test_labels.extend(labels.tolist())\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "print(f\"Test Loss {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5325e19-2edd-4732-8732-e6b679fe7561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Actual')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(test_predictions, test_labels)\n",
    "plt.plot([4, 8], [4, 8], 'k--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d3af7",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cbc7171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./chemberta_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# Define save directory\n",
    "save_directory = \"./chemberta_finetuned_model\"\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c4368",
   "metadata": {},
   "source": [
    "## Loading full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfffb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38725205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "      (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.144, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.109, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.144, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b43db3",
   "metadata": {},
   "source": [
    "## Predict on new smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8def1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "      (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.144, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.109, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.144, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f32a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize a new SMILES string\n",
    "def tokenize_smiles(smiles, tokenizer, max_length=512):\n",
    "    encoding = tokenizer(\n",
    "        smiles,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "new_smiles = \"[H]O[H]\"  # <-- Third row in df, true value is -10.370616\t\n",
    "input_ids, attention_mask = tokenize_smiles(new_smiles, tokenizer)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35b331d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for SMILES '[H]O[H]': -82.7963\n"
     ]
    }
   ],
   "source": [
    "# 3. Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    prediction = outputs.logits.squeeze().item()  # For regression output\n",
    "\n",
    "print(f\"Predicted value for SMILES '{new_smiles}': {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464e734",
   "metadata": {},
   "source": [
    "# Again but with early stopping + learning rate scheduler + gradient clipper (for stability) + extra logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss criterion and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training parameters\n",
    "# epochs = 10\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 3\n",
    "# patience_counter = 0\n",
    "\n",
    "# torch.manual_seed(12345)\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "#         output_dict = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         predictions = output_dict.logits.squeeze(dim=1)\n",
    "#         loss = criterion(predictions, labels)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "#         total_train_loss += loss.item()\n",
    "    \n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             input_ids, attention_mask, labels = batch\n",
    "#             input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "#             output_dict = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             predictions = output_dict.logits.squeeze(dim=1)\n",
    "#             loss = criterion(predictions, labels)\n",
    "#             total_val_loss += loss.item()\n",
    "    \n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     print(f\"Epoch {epoch + 1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}\")\n",
    "    \n",
    "#     # Learning rate scheduling\n",
    "#     scheduler.step(avg_val_loss)\n",
    "    \n",
    "#     # Early stopping\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         patience_counter = 0\n",
    "#         torch.save(model.state_dict(), 'best_chemberta_model.pth')\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             break\n",
    "\n",
    "# print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom_sys_qm9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
